{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naivebayes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78zNROnxHfJJ"
      },
      "source": [
        "# Import libraries and dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bb9rRGkqDPF"
      },
      "source": [
        "We import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go39gIe-u1Qf"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYvWJ5Rqp_BU"
      },
      "source": [
        "We load the IMDB dataset from Huggingface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU2IJ-hqu3T3",
        "outputId": "64594836-0f6b-41ca-e2d6-67291d0f1283"
      },
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset_train = load_dataset('imdb', split='train')\n",
        "dataset_test = load_dataset('imdb', split='test')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.12.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.4.post0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: ruamel.yaml==0.17.16 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (0.17.16)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.0.12)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from ruamel.yaml==0.17.16->huggingface-hub<0.1.0,>=0.0.14->datasets) (0.2.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.6.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f48LA3iXHiLE"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuiEvhrTgBqK"
      },
      "source": [
        "We're going to do some preprocessing for the testing part of our algorithm later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wngNmt6Y922L"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# loading the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"lemmatizer\", \"ner\"])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbJkjKIIqMP-"
      },
      "source": [
        "From the dataset we create our training and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqrkf8Fdu5Jm",
        "outputId": "1b274d96-bb0e-4aa1-81a3-ff11b1482489"
      },
      "source": [
        "x_train, y_train, x_test, y_test = dataset_train[:]['text'], dataset_train[:]['label'], dataset_test[:]['text'], dataset_test[:]['label']\n",
        "len(x_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_vBxXs1u6y6",
        "outputId": "08dc8ba0-8981-4efe-f016-cf0f90b51f76"
      },
      "source": [
        "# Remove\n",
        "\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "\n",
        "def stem(l):\n",
        "    '''\n",
        "\n",
        "    '''\n",
        "    res = []\n",
        "    re_word = re.compile(r\"^\\w+$\")\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    for text in tqdm(l, total=len(l)):\n",
        "      res.append(\" \".join([stemmer.stem(word) for word in word_tokenize(text.lower()) if re_word.match(word)]))\n",
        "    return res"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKzP9sD1u8ZE",
        "outputId": "8293ea7e-7fbf-4b1e-8405-fbbdd32454d2"
      },
      "source": [
        "# We apply stemming to our dataset\n",
        "stemmed_train = stem(x_train)\n",
        "stemmed_test = stem(x_test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25000/25000 [02:03<00:00, 202.88it/s]\n",
            "100%|██████████| 25000/25000 [02:00<00:00, 207.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hUMMLFmwFLr"
      },
      "source": [
        "# Remove\n",
        "\n",
        "def lemm(l):\n",
        "    '''\n",
        "  \n",
        "    '''\n",
        "    lemmas = []\n",
        "    re_word = re.compile(r\"^\\w+$\")\n",
        "    for text in tqdm(l, total=len(l)):\n",
        "      lemmas.append(' '.join([token.lemma_ for token in nlp(text.lower()) if re_word.match(token.text)]))\n",
        "    return lemmas"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvtngXJWwGv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81fd2f11-1807-472b-e5ac-3063a3815a2d"
      },
      "source": [
        "# We apply lemmatization to our dataset\n",
        "\n",
        "lemmas_train = lemm(x_train)\n",
        "lemmas_test = lemm(x_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25000/25000 [00:55<00:00, 447.32it/s]\n",
            "100%|██████████| 25000/25000 [00:53<00:00, 466.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96csb5wTwNI6",
        "outputId": "5466e80a-dedf-42de-9371-2e5ce287f79b"
      },
      "source": [
        "# We create a dataframe version of our dataset because it will be easier to\n",
        "# use later on when we want to test\n",
        "\n",
        "df_train_lemma = pd.DataFrame(list(zip(lemmas_train, y_train)), columns=['val', 'label'])\n",
        "df_test_lemma = pd.DataFrame(list(zip(lemmas_test, y_test)), columns=['val', 'label'])\n",
        "\n",
        "df_train_stem = pd.DataFrame(list(zip(stemmed_train, y_train)), columns=['val', 'label'])\n",
        "df_test_stem = pd.DataFrame(list(zip(stemmed_test, y_test)), columns=['val', 'label'])\n",
        "\n",
        "df_train = pd.DataFrame(list(zip(x_train, y_train)), columns=['val', 'label'])\n",
        "df_test = pd.DataFrame(list(zip(x_test, y_test)), columns=['val', 'label'])\n",
        "df_train.head(3), df_test.head(3)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                                 val  label\n",
              " 0  Bromwell High is a cartoon comedy. It ran at t...      1\n",
              " 1  Homelessness (or Houselessness as George Carli...      1\n",
              " 2  Brilliant over-acting by Lesley Ann Warren. Be...      1,\n",
              "                                                  val  label\n",
              " 0  I went and saw this movie last night after bei...      1\n",
              " 1  Actor turned director Bill Paxton follows up h...      1\n",
              " 2  As a recreational golfer with some knowledge o...      1)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9watg-fHHs9m"
      },
      "source": [
        "# Naive Bayes algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh82NNnjVDmg"
      },
      "source": [
        "# Typing to make functions clearer\n",
        "from typing import List\n",
        "from typing import Dict\n",
        "from typing import Tuple\n",
        "\n",
        "# Functions used in our Naive Bayes algorithm\n",
        "\n",
        "def occurences_and_vocabulary(x_train: List[str], \n",
        "                              y_train: List[str], \n",
        "                              classes: List[int]) -> Tuple[Dict[int, Dict[str, int]], List[str]]:\n",
        "    '''\n",
        "    Takes the input dataset and create a list of every word found in it to form\n",
        "    the vocabulary\n",
        "    From this dataset we also create a dictionnary compiling the occurence of\n",
        "    every word found in the given dataset according to the class of the current\n",
        "    document\n",
        "    '''\n",
        "    dictionnary = {}\n",
        "    # We initialize our dictionnary with as much entry as there are classes\n",
        "    # This will be a dictionnary of dictionnaries, with each key a class\n",
        "    # Each dictionnary will record the occurence of words for a given class\n",
        "    for c in classes:\n",
        "      dictionnary[c] = {}\n",
        "    c = -1\n",
        "    vocabulary = []\n",
        "    for i in range(len(y_train)):\n",
        "      c = y_train[i]\n",
        "      # We split our document into token, each token is a word from the document\n",
        "      splitted_doc = re.split(\"[ .,\\\"]\", x_train[i])\n",
        "      for word in splitted_doc:\n",
        "        vocabulary.append(word)\n",
        "        # We count the number of occurence of the current word in all of the\n",
        "        # given dataset\n",
        "        if word not in dictionnary[c]:\n",
        "          dictionnary[c][word] = 1\n",
        "        else:\n",
        "          dictionnary[c][word] += 1\n",
        "    # We remove multiple occurences of words in our vocabulary\n",
        "    vocabulary = np.unique(vocabulary)\n",
        "\n",
        "    return dictionnary, vocabulary\n",
        "\n",
        "def sum_counts(D: Dict[int, Dict[str, int]], classes: List[int]) -> List[int]:\n",
        "    '''\n",
        "    Counts the total number of word found in the dataset for each class\n",
        "    '''\n",
        "    sum_per_class = [0 for i in range(len(classes))]\n",
        "    for key in D:\n",
        "      # Since we have a dictionnary of dictionnary as input, we take the values\n",
        "      # of the current dictionnary and sum all of them\n",
        "      sum_per_class[key] = sum(D[key].values())\n",
        "    return sum_per_class"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1Wh9UBY8Tx6"
      },
      "source": [
        "# Function used for the binary Naive Bayes\n",
        "\n",
        "def word_check_and_vocabulary(x_train: List[str], \n",
        "                              y_train: List[str], \n",
        "                              classes: List[int]) -> Tuple[Dict[int, Dict[str, int]], List[str]]:\n",
        "    '''\n",
        "    Takes the input dataset and create a list of every word found in it to form\n",
        "    the vocabulary\n",
        "    From this dataset we also create a dictionnary compiling which word can be\n",
        "    found in which class of document\n",
        "    '''\n",
        "    # We still retain the dictionnary of dictionnaries format to keep the same\n",
        "    # code structure\n",
        "    word_check = {}\n",
        "    for c in classes:\n",
        "      word_check[c] = {}\n",
        "    c = -1\n",
        "    vocabulary = []\n",
        "    for i in range(len(y_train)):\n",
        "      c = y_train[i]\n",
        "      splitted_doc = re.split(\"[ .,\\\"]\", x_train[i])\n",
        "      for word in splitted_doc:\n",
        "        vocabulary.append(word)\n",
        "        # Instead of adding, we just keep the occurence at 1\n",
        "        if word not in word_check[c]:\n",
        "          word_check[c][word] = 1\n",
        "    vocabulary = np.unique(vocabulary)\n",
        "\n",
        "    return word_check, vocabulary"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb4OcCYl-sUZ"
      },
      "source": [
        "def train_naive_bayes(data: List[str], target: List[int]) -> Tuple[List[int], Dict[int, Dict[str, int]], List[str]]:\n",
        "    '''\n",
        "    Take an input dataset and train the model on that dataset, generating\n",
        "    the likelihood of each word with respect to each class as well as\n",
        "    a vocabulary of the training dataset\n",
        "    '''\n",
        "    # We initialize our data\n",
        "    logprior = dict()\n",
        "    classes = np.unique(target)\n",
        "    count, vocabulary = occurences_and_vocabulary(data, target, classes)\n",
        "    ndoc = len(target)\n",
        "    loglikelihood = dict()\n",
        "    sum_per_class = sum_counts(count, classes)\n",
        "\n",
        "    # For each class we compute the loglikelihood of every word present in\n",
        "    # document of that class\n",
        "    for c in classes:\n",
        "      nc = np.count_nonzero(target == c)\n",
        "      logprior[c] = np.log(nc/ndoc)\n",
        "\n",
        "      # For each word found in a certain class, we compute the\n",
        "      # word's loglikelihood\n",
        "      loglikelihood[c] = {}\n",
        "      for key in count[c]:\n",
        "        loglikelihood[c][key] = np.log((count[c][key] + 1) / (sum_per_class[c] + 1))\n",
        "\n",
        "    return logprior, loglikelihood, vocabulary"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgzjHlqoxpxD"
      },
      "source": [
        "def train_binary_naive_bayes(data: List[str], target: List[int]) -> Tuple[Dict[int, Dict[str, int]], List[str]]:\n",
        "    '''\n",
        "    Take an input dataset and record which word appear in which document\n",
        "    of which class\n",
        "    '''\n",
        "    logprior = dict()\n",
        "    classes = np.unique(target)\n",
        "    count, vocabulary = word_check_and_vocabulary(data, target, classes)\n",
        "    return count, vocabulary"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfOXpro25Hka"
      },
      "source": [
        "# Test one doc at a time\n",
        "\n",
        "def test_naive_bayes(testdoc: List[str], \n",
        "                     logprior: List[int], \n",
        "                     loglikelihood: Dict[int, Dict[str, int]], \n",
        "                     C: List[int]) -> int:\n",
        "    '''\n",
        "    Takes the given input and from the given logprior, loglikelihood\n",
        "    we evaluate the class of that input \n",
        "    '''\n",
        "    sum_ = [0 for i in range(len(logprior))]\n",
        "    for c in C:\n",
        "        sum_[c] = logprior[c]\n",
        "        for word in testdoc:\n",
        "          # We do a try except to avoid doing if checks for every word\n",
        "          # and speed up the process\n",
        "          # If we go into the except condition, that means the word is\n",
        "          # not present so we do not take it into account\n",
        "          try:\n",
        "            sum_[c] += loglikelihood[c][word]\n",
        "          except:\n",
        "            pass\n",
        "    return np.argmax(sum_)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft41wmOO9MNU"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a111CQm7-Bct"
      },
      "source": [
        "First, let's see how accurate our model is when trained with the initial dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbmgYUjM1_uB"
      },
      "source": [
        "logprior, loglikelihood, vocabulary = train_naive_bayes(x_train, y_train)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nm9evuW-Rba",
        "outputId": "11974b3b-12df-4854-d02a-005880c7cb03"
      },
      "source": [
        "accuracy = 0\n",
        "# Create a confusion_matrix\n",
        "confusion_matrix = np.array([[0, 0], [0, 0]])\n",
        "# We transform our documents into a list of words\n",
        "split_df = df_test['val'].str.split(\"[ .,\\\"]\")\n",
        "# We test 1 document at a time\n",
        "for i in range(len(split_df)):\n",
        "    # The model predict the current document's class\n",
        "    res = test_naive_bayes(split_df[i], logprior, loglikelihood, [0, 1])\n",
        "    if res == y_test[i]:\n",
        "        accuracy += 1\n",
        "    confusion_matrix[res, y_test[i]] += 1\n",
        "accuracy /= len(split_df)\n",
        "accuracy"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.61016"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6tD_gCPnDtM"
      },
      "source": [
        "It's not very accurate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1q09y8pdFlW",
        "outputId": "b5fcc0f5-9ed7-411f-aa7d-4f707cd89cc3"
      },
      "source": [
        "# To note, because 1 is positive and 0 is negative here, positions are reversed\n",
        "# True negative is at position (0, 0)\n",
        "# False negative at (0, 1)\n",
        "# False positive at (1, 0)\n",
        "# True positive at (1, 1)\n",
        "\n",
        "print(\"Confusion matrix: \")\n",
        "print(matrix)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix:\n",
            "[[7267 4513]\n",
            " [5233 7987]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If5TDFjLindc"
      },
      "source": [
        "From the confusion matrix we can note that the model often thinks the review is a positive one when it's a negative review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIuoNFxlnGCm"
      },
      "source": [
        "## With the different preprocessing methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5JO8N9f-1mF"
      },
      "source": [
        "Let's add some preprocessing and see how it improves our model.\n",
        "\n",
        "First, let's see how stemming improve our accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvdfKrGi-1Ia"
      },
      "source": [
        "# Preprocessing has already been done beforehand, we now use this stemmed dataset\n",
        "logprior_stemming, loglikelihood_stemming, vocabulary_stemming = train_naive_bayes(stemmed_train, y_train)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F7Cj7Rx_TXb",
        "outputId": "63fd14f9-fa8e-4500-bc87-2b66618056f3"
      },
      "source": [
        "accuracy_stemming = 0\n",
        "confusion_matrix_stem = np.array([[0, 0], [0, 0]])\n",
        "split_df = df_test_stem['val'].str.split(\"[ .,\\\"]\")\n",
        "for i in range(len(split_df)):\n",
        "    res = test_naive_bayes(split_df[i], logprior_stemming, loglikelihood_stemming, [0, 1])\n",
        "    if res == y_test[i]:\n",
        "        accuracy_stemming += 1\n",
        "    confusion_matrix_stem[res, y_test[i]] += 1\n",
        "accuracy_stemming /= len(split_df)\n",
        "accuracy_stemming"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.70656"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z7BFuoFixgw",
        "outputId": "ef2d2a1e-d1b7-4536-b050-8f2f1498b701"
      },
      "source": [
        "print(\"Confusion matrix: \")\n",
        "print(confusion_matrix_stem)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix: \n",
            "[[9631 4467]\n",
            " [2869 8033]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liPY6C74__l2"
      },
      "source": [
        "As we can see, the accuracy increased by 10% which is a lot.\n",
        "This is explained by a much smaller vocabulary, so there's a lot less word to count which makes the whole process less messy.\n",
        "\n",
        "As we can see:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bHzHGUYErpS",
        "outputId": "e7d90edf-b372-4318-b191-1f0bdeb1f2cb"
      },
      "source": [
        "print(\"Length vocabulary: \" + str(len(vocabulary)))\n",
        "print(\"Length vocabulary after stemming: \" + str(len(vocabulary_stemming)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length vocabulary: 177907\n",
            "Length vocabulary after stemming: 49054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tpyk0swVFhPC"
      },
      "source": [
        "The length of the vocabulary after stemming is about 1/4 the size of the original vocabulary.\n",
        "\n",
        "Let's see with lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud9Fd6i0Cykz"
      },
      "source": [
        "# Preprocessing has already been done beforehand, we now use this lemmatized dataset\n",
        "logprior_lemma, loglikelihood_lemma, vocabulary_lemma = train_naive_bayes(lemmas_train, y_train)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btRxXFA4C0Wl",
        "outputId": "4f1ae055-ab73-4c62-f65f-4c913c037511"
      },
      "source": [
        "accuracy_lemma = 0\n",
        "confusion_matrix_lemma = np.array([[0, 0], [0, 0]])\n",
        "split_df = df_test_lemma['val'].str.split(\"[ .,\\\"]\")\n",
        "for i in range(len(split_df)):\n",
        "    res = test_naive_bayes(split_df[i], logprior_lemma, loglikelihood_lemma, [0, 1])\n",
        "    if res == y_test[i]:\n",
        "        accuracy_lemma += 1\n",
        "    confusion_matrix_lemma[res, y_test[i]] += 1\n",
        "accuracy_lemma /= len(split_df)\n",
        "accuracy_lemma"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.69368"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipU52mjGjl28",
        "outputId": "b60d37bc-2263-4a4f-ee83-f3f85ba57f22"
      },
      "source": [
        "print(\"Confusion matrix: \")\n",
        "print(confusion_matrix_lemma)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix: \n",
            "[[9474 4632]\n",
            " [3026 7868]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjP-dIK6GX8T"
      },
      "source": [
        "Slightly less accurate than with stemming, but still a net improvement over no pre-processing at all. The accuracy with lemmatization would be improved further with a more detailed vocabulary.\n",
        "\n",
        "We can also notice that with the preprocessing, the model now has a much easier time understanding when a review is negative. This can be because after stemming or lemmatization, a lot of words can be found in both positive and negative reviews but negative reviews will have a lot more negative words like \"not\" standing out in terms of occurences. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zLxktnLNo5r"
      },
      "source": [
        "## Comparing with Binary Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB27znwUHPpd"
      },
      "source": [
        "Let's compare the accuracy between counting the occurence of words and just checking their presence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUtgmuwm8o4I"
      },
      "source": [
        "word_check, binary_vocabulary = train_binary_naive_bayes(x_train, y_train)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA7cjq4x_cnA"
      },
      "source": [
        "accuracy_binary = 0\n",
        "confusion_matrix_binary = np.array([[0, 0], [0, 0]])\n",
        "split_df = df_test['val'].str.split(\"[ .,\\\"]\")\n",
        "for i in range(len(split_df)):\n",
        "    res = test_naive_bayes(split_df[i], [0, 0], word_check, [0, 1])\n",
        "    if res == y_test[i]:\n",
        "        accuracy_binary += 1\n",
        "    confusion_matrix_binary[res, y_test[i]] += 1\n",
        "accuracy_binary /= len(split_df)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0uYvpj-HhLs",
        "outputId": "15fb23ad-7e40-456d-c032-733e9447e348"
      },
      "source": [
        "print(\"Naive Bayes accuracy: \" + str(accuracy))\n",
        "print(\"Binary Naive Bayes accuracy: \" + str(accuracy_binary))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes accuracy: 0.61016\n",
            "Binary Naive Bayes accuracy: 0.5908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CTQcBsSjqkH",
        "outputId": "49216c8f-819f-4bcd-dd3f-7bb6ee670a8b"
      },
      "source": [
        "print(\"Confusion matrix: \")\n",
        "print(confusion_matrix_binary)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix: \n",
            "[[9615 7345]\n",
            " [2885 5155]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wK_7FLEHxB4"
      },
      "source": [
        "As we can see, just checking their presence results in a lower accuracy and this is due to a lack of depth in learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIdcLDYONsSH"
      },
      "source": [
        "## What happened when our model guessed incorrectly ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSaUlB3KMGjs"
      },
      "source": [
        "# To test our model on-the-fly\n",
        "split_df = df_test_stem['val'].str.split(\"[ .,\\\"]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txQUB59ZJTMV"
      },
      "source": [
        "Let's take a look at some instances where our model wrongly determined the positivity of the document. We'll use our model with stemming because it has the best accuracy out of all models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKMden4OHvgQ",
        "outputId": "a1468cc1-0279-4a5c-9723-479116c0ccbe"
      },
      "source": [
        "i = 7 # Arbitrary number\n",
        "var = test_naive_bayes(split_df[i], logprior_stemming, loglikelihood_stemming, [0, 1])\n",
        "print(\"Model's prediction: \" + str(var))\n",
        "print(\"Answer: \" + str(y_test[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's prediction: 0\n",
            "Answer: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-yK6n8kMUHZ"
      },
      "source": [
        "Our model thought it was a negative critic but it was in fact a positive one. Let's take a look at the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "8MQT8LQaMVOk",
        "outputId": "5cef44a0-3404-4d9b-a738-1cbf3fa8f98b"
      },
      "source": [
        "df_test_stem['val'][i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i felt this film did have mani good qualiti the cinematographi was certain differ expos the stage aspect of the set and stori the origin charact as actor was certain an achiev and i felt most play quit convinc of cours they are play themselv but definit uniqu the cultur aspect may leav mani disappoint as a familiar with the chines and orient cultur will answer a lot of question regard relationship and the stigma that goe with ani drug use i found the jia hongsheng stori interest on a down note the stori is in beij and some of the fashion and music reek of earli 90s even though this was made in 2001 so it realli cheesi sometim the beatl crap etc whatev not a top ten or twenti but if it on the televis check it out'"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WklVO4RFIyWn"
      },
      "source": [
        "When analizing this document, we can notice a lot of words that could be counted as negative such as \"reek\", \"crap, \"drug\", \"disappoint\" etc. which probably have mislead our model into thinking this was a negative review.\n",
        "\n",
        "Let's take a look at another document our model failed to assess correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkyGo203IJbq",
        "outputId": "5caee704-ba04-467d-e188-263cd2cce770"
      },
      "source": [
        "i = 62\n",
        "split_df = df_test_stem['val'].str.split(\"[ .,\\\"]\")\n",
        "var = test_naive_bayes(split_df[62], logprior_stemming, loglikelihood_stemming, [0, 1])\n",
        "print(\"Model's prediction: \" + str(var))\n",
        "print(\"Answer: \" + str(y_test[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's prediction: 0\n",
            "Answer: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC1zJdulL0vb"
      },
      "source": [
        "Once again, our model thought it was a negative critic but it was a positive one. Let's take a look at the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "DbXsAO-nKFn3",
        "outputId": "4dc46262-f11d-480e-e2d1-43bba63014e3"
      },
      "source": [
        "df_test_stem['val'][i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i have been a fan of madonna for quit sometim now howev i thought i would comment on this br br this film mistaken one of them as well as madonna was pan by the critic they were high mistaken and mani potenti viewer were turn off by the bad br br first madonna doe an excel job in this movi which was one of her first she play a ditsi blond in the film she is far from a ditsi blond in real life most critic were somewhat prejud by her sing fame and did give her a fair shake when you view this film i hope that you understand that the accent and the goofi is just act she was absolut hyster as was the br br griffen dunn is anoth person who was not given a fair review in the film if you take a look at his filmographi you will see he is quit an accomplish br br as far as the movi itself this is someth similar to pretti woman but came 3 year befor the robert gere success it a comedi with lot of site gag slapstick and one liner some of the comedi is deadpan and take a comedi aficionado to realli appreci the more subtl br br i know this doe tell you much about the movi howev i hope this help dispel ani belief that this is a poor movi it is absolut worth rent for an enjoy night of great br br br br gari'"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks2npcUfLTlr"
      },
      "source": [
        "The same thing can be said here: we can see words like \"mistaken\", \"critic\", \"turn off\", \"goofi\" be very present in this document which probably have mislead our model into thinking it was a negative critic instead of a more positive one."
      ]
    }
  ]
}