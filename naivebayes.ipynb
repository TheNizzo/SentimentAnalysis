{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "go39gIe-u1Qf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nU2IJ-hqu3T3",
    "outputId": "ae299298-faac-4e26-e6f5-fbb67c30ecc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.12.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.10.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.17)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a...\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "from datasets import load_dataset\n",
    "dataset_train = load_dataset('imdb', split='train')\n",
    "dataset_test = load_dataset('imdb', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wngNmt6Y922L"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# loading the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"lemmatizer\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqrkf8Fdu5Jm"
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = dataset_train[:]['text'], dataset_train[:]['label'], dataset_test[:]['text'], dataset_test[:]['label']\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_vBxXs1u6y6"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "\n",
    "def stem(l):\n",
    "  res = []\n",
    "  re_word = re.compile(r\"^\\w+$\")\n",
    "  stemmer = SnowballStemmer(\"english\")\n",
    "  for text in tqdm(l, total=len(l)):\n",
    "    res.append(\" \".join([stemmer.stem(word) for word in word_tokenize(text.lower()) if re_word.match(word)]))\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKzP9sD1u8ZE"
   },
   "outputs": [],
   "source": [
    "stemmed_train = stem(x_train)\n",
    "stemmed_test = stem(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hUMMLFmwFLr"
   },
   "outputs": [],
   "source": [
    "def lemm(l):\n",
    "  lemmas = []\n",
    "  re_word = re.compile(r\"^\\w+$\")\n",
    "  for text in tqdm(l, total=len(l)):\n",
    "    lemmas.append(' '.join([token.lemma_ for token in nlp(text.lower()) if re_word.match(token.text)]))\n",
    "  return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvtngXJWwGv6"
   },
   "outputs": [],
   "source": [
    "lemmas_train = lemm(x_train)\n",
    "lemmas_test = lemm(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3oZuKq75wKP9",
    "outputId": "be0eff74-5997-4137-aec9-3dad0159530b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-03 12:44:58--  https://raw.githubusercontent.com/cjhutto/vaderSentiment/master/vaderSentiment/vader_lexicon.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 426786 (417K) [text/plain]\n",
      "Saving to: ‘vader_lexicon.txt.1’\n",
      "\n",
      "\r",
      "vader_lexicon.txt.1   0%[                    ]       0  --.-KB/s               \r",
      "vader_lexicon.txt.1 100%[===================>] 416.78K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-10-03 12:44:58 (11.9 MB/s) - ‘vader_lexicon.txt.1’ saved [426786/426786]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/cjhutto/vaderSentiment/master/vaderSentiment/vader_lexicon.txt\n",
    "lexicon = pd.read_csv(\"vader_lexicon.txt\", sep=\"\\t\", names=['word', 'MEAN-SENTIMENT-RATING', 'a', 'b']).drop(['a', 'b'], axis = 'columns')\n",
    "d = {}\n",
    "for w, v in lexicon.iterrows():\n",
    "    d[v[0]] = v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juijQgvdLbeV"
   },
   "outputs": [],
   "source": [
    "def occurences_and_vocabulary(x_train, y_train, classes):\n",
    "  dictionnary = {}\n",
    "  for c in classes:\n",
    "    dictionnary[c] = {}\n",
    "  c = -1\n",
    "  vocabulary = []\n",
    "  for i in range(len(y_train)):\n",
    "    c = y_train[i]\n",
    "    splitted_doc = re.split(\"[ .,\\\"]\", x_train[i])\n",
    "    for word in splitted_doc:\n",
    "      vocabulary.append(word)\n",
    "      if word not in dictionnary[c]:\n",
    "        dictionnary[c][word] = 1\n",
    "      else:\n",
    "        dictionnary[c][word] += 1\n",
    "  return dictionnary, vocabulary\n",
    "\n",
    "# dictionnary, vocabulary = occurences_and_vocabulary(lemmas_train, y_train, [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yh82NNnjVDmg"
   },
   "outputs": [],
   "source": [
    "def sum_counts(D):\n",
    "    dicts = D.values()\n",
    "    iterator = iter(dicts)\n",
    "    sum_pos = sum(next(iterator).values())\n",
    "    sum_neg = sum(next(iterator).values())\n",
    "    return sum_pos, sum_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eb4OcCYl-sUZ",
    "outputId": "2f3b0c55-6016-42ae-bc10-dc4b7a38c79f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "12500\n",
      "25000\n",
      "1\n",
      "12500\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "def train_naive_bayes(D, C):\n",
    "    logprior = dict()\n",
    "    bigdoc = dict()\n",
    "    (data, target) = D\n",
    "    count, vocabulary = occurences_and_vocabulary(data, target, C)\n",
    "    ndoc = len(target)\n",
    "    loglikelihood = dict()\n",
    "    sum_pos, sum_neg = sum_counts(count)\n",
    "    for c in C:\n",
    "#       nc = np.count_nonzero(target)\n",
    "      nc = 12500 # for testing\n",
    "      logprior[c] = np.log(nc/ndoc)\n",
    "      for w in vocabulary:\n",
    "          if not c in loglikelihood:\n",
    "              loglikelihood[c] = {}\n",
    "              # somme des counts de w dans V + 1\n",
    "          if w not in count[c]:\n",
    "              loglikelihood[c][w] = 0\n",
    "              continue\n",
    "          if c == 0:\n",
    "              loglikelihood[c][w] = np.log((count[c][w] + 1) / (sum_pos + 1))\n",
    "          else:\n",
    "              loglikelihood[c][w] = np.log((count[c][w] + 1) / (sum_neg + 1))\n",
    "    return logprior, loglikelihood, vocabulary\n",
    "\n",
    "\n",
    "logprior, loglikelihood, vocabulary = train_naive_bayes2((lemmas_train, y_train), [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfOXpro25Hka"
   },
   "outputs": [],
   "source": [
    "def test_naive_bayes(testdoc, logprior, loglikelihood, C, V):\n",
    "    sum_ = [0, 0]\n",
    "    for c in C:\n",
    "        sum_[c] = logprior[c]\n",
    "        for word in testdoc:\n",
    "            if word in V:\n",
    "                sum_[c] += loglikelihood[c][word]\n",
    "    return np.argmax(sum_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "v2fD4FQw5JbX",
    "outputId": "82248c2d-765d-4ae1-e018-9b2534faf5b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[-5585.247989409187, -5678.753159740923]\n",
      "1\n",
      "[-13958.217882890132, -14216.797873996173]\n",
      "2\n",
      "[-10021.002107287055, -10182.291490542606]\n",
      "3\n",
      "[-4677.760572049008, -4763.57465068901]\n",
      "4\n",
      "[-8651.044405629782, -8803.517715673326]\n",
      "5\n",
      "[-12409.14639399683, -12640.862393161175]\n",
      "6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-e77b99d09d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_naive_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloglikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-91070b9e0c9d>\u001b[0m in \u001b[0;36mtest_naive_bayes\u001b[0;34m(testdoc, logprior, loglikelihood, C, V)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msum_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogprior\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0msum_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloglikelihood\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for i in range(len(x_test)):\n",
    "    print(i)\n",
    "    var = test_naive_bayes(lemmas_test[i], logprior, loglikelihood, [0, 1], vocabulary)\n",
    "    if var == y_test[i]:\n",
    "        accuracy += 1\n",
    "accuracy /= len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8qpyMIVd3vi"
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "naivebayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
